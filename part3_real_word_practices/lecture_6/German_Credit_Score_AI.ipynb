{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# German credit score\n",
        "\n",
        "\n",
        "## Dataset Overview\n",
        "\n",
        "The original dataset contains 1000 entries with 20 categorical/symbolic attributes prepared by Prof. Hofmann. In this dataset, each entry represents a person who takes a credit from a bank. Each person is classified as either a good or bad credit risk according to the set of attributes.\n",
        "\n",
        "The link to the original dataset can be found below.\n",
        "\n",
        "## Content\n",
        "\n",
        "It is almost impossible to understand the original dataset due to its complicated system of categories and symbols. Therefore, I wrote a small Python script to convert it into a readable CSV file. Several columns were ignored because, in my opinion, either they are not important or their descriptions are unclear.\n",
        "\n",
        "The selected attributes are:\n",
        "\n",
        "- **Age** (numeric)\n",
        "- **Sex** (text: male, female)\n",
        "- **Job** (numeric:\n",
        "  - 0 - unskilled and non-resident\n",
        "  - 1 - unskilled and resident\n",
        "  - 2 - skilled\n",
        "  - 3 - highly skilled)\n",
        "- **Housing** (text: own, rent, or free)\n",
        "- **Saving accounts** (text: little, moderate, quite rich, rich)\n",
        "- **Checking account** (numeric, in DM - Deutsch Mark)\n",
        "- **Credit amount** (numeric, in DM)\n",
        "- **Duration** (numeric, in months)\n",
        "- **Purpose** (text: car, furniture/equipment, radio/TV, domestic appliances, repairs, education, business, vacation/others)\n",
        "- **Risk** (target value: Good or Bad Risk)\n"
      ],
      "metadata": {
        "id": "uG4pmM3gntxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -rf credit_scoring_7904_Q4_2024\n",
        "!git clone https://github.com/cesarlegendre/credit_scoring_7904_Q4_2024\n"
      ],
      "metadata": {
        "id": "qSLJ0PEZqw6Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8faca91d-7472-41e4-ebef-9152c30d3a32"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'credit_scoring_7904_Q4_2024'...\n",
            "remote: Enumerating objects: 202, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 202 (delta 2), reused 1 (delta 1), pack-reused 199 (from 1)\u001b[K\n",
            "Receiving objects: 100% (202/202), 2.43 MiB | 6.80 MiB/s, done.\n",
            "Resolving deltas: 100% (82/82), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "#loading dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "# data splitting\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# data modeling\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Mondel performance\n",
        "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "#warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore')\n",
        "\n",
        "\n",
        "file = 'credit_scoring_7904_Q4_2024/data_sets/german_credit_score/data.csv'\n"
      ],
      "metadata": {
        "id": "FeCd9H6nnvLl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Strategy for Data Analysis and Model Development\n",
        "\n",
        "#### 1. Introduction\n",
        "   - **Objective**: Provide a clear overview of the dataset and its context.\n",
        "   - **Action**:\n",
        "     - Describe the source of the dataset.\n",
        "     - Provide key details about the dataset (size, attributes, and target variable).\n",
        "\n",
        "#### 2. Library Setup\n",
        "   - **Objective**: Ensure that the necessary tools are available for data processing and analysis.\n",
        "   - **Action**:\n",
        "     - Import essential libraries (e.g., pandas, numpy, matplotlib, seaborn, scikit-learn).\n",
        "     - Load the dataset into a usable format (e.g., CSV, database, etc.).\n",
        "\n",
        "#### 3. Data Familiarization\n",
        "   - **Objective**: Understand the structure and nature of the dataset.\n",
        "   - **Action**:\n",
        "     - **3.1 Data Types**: Examine the types of each attribute.\n",
        "     - **3.2 Data Shape**: Understand the dimensions (rows, columns) of the dataset.\n",
        "     - **3.3 Missing Data**: Identify any null values and their distribution across attributes.\n",
        "     - **3.4 Unique Values**: Check for unique or categorical values in each column.\n",
        "     - **3.5 Initial Inspection**: Display the first few rows to get a sense of the data.\n",
        "\n",
        "#### 4. Variable Exploration\n",
        "   - **Objective**: Dive deeper into key variables to gain insights.\n",
        "   - **Action**:\n",
        "     - **4.1 Descriptive Statistics and Visualizations**:\n",
        "       - Use histograms, bar charts, and boxplots to explore distributions.\n",
        "       - Summarize key descriptive statistics (mean, median, mode, variance, etc.).\n",
        "\n",
        "#### 5. Data Correlation\n",
        "   - **Objective**: Identify relationships between variables.\n",
        "   - **Action**:\n",
        "     - **5.1 Correlation Analysis**:\n",
        "       - Compute correlation matrices.\n",
        "       - Visualize correlations with heatmaps to detect highly correlated variables.\n",
        "\n",
        "#### 6. Data Preprocessing\n",
        "   - **Objective**: Prepare data for model training.\n",
        "   - **Action**:\n",
        "     - **6.1 Preprocessing Libraries**: Import necessary libraries for preprocessing (e.g., StandardScaler, LabelEncoder).\n",
        "     - **6.2 Defining Features and Target**: Separate the dataset into features (X) and target (Y) variables.\n",
        "     - **6.3 Train-Test Split**: Split the data into training and testing sets for model validation.\n",
        "\n",
        "#### 7. Model Development and Evaluation\n",
        "\n",
        "   **7.1 Model 1: Random Forest**\n",
        "   - **Objective**: Build and evaluate the first model using Random Forest.\n",
        "   - **Action**:\n",
        "     - **7.1.1 Random Forest Implementation**: Train a Random Forest model on the training set.\n",
        "     - **7.1.2 Model Scoring**: Evaluate the model's accuracy, precision, recall, and other metrics.\n",
        "     - **7.1.3 Cross Validation**: Apply cross-validation to ensure the model's generalizability.\n",
        "\n",
        "   **7.2 Model 2: Logistic Regression**\n",
        "   - **Objective**: Build and evaluate the second model using Logistic Regression.\n",
        "   - **Action**:\n",
        "     - **7.2.1 Logistic Regression Implementation**: Train a Logistic Regression model on the training set.\n",
        "     - **7.2.2 Model Scoring**: Evaluate the model's performance (accuracy, precision, recall, F1-score).\n",
        "     - **7.2.3 Cross Validation**: Validate the model using cross-validation techniques.\n",
        "     - **7.2.4 ROC Curve**: Plot the ROC curve to assess the model's ability to distinguish between classes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0a6HlYAQuJOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompting step by step, DIY\n",
        "\n",
        "\n",
        "1. **Instruction**: Load required libraries and import dataset.\n",
        "   **Prompt**: \"Write Python code to load libraries like pandas, numpy, seaborn, matplotlib, and load a CSV dataset into a DataFrame using pandas.\"\n",
        "\n",
        "2. **Instruction**: Check for missing values, data types, and the shape of the dataset.\n",
        "   **Prompt**: \"Write Python code to display the data types, check for missing values, and show the shape of a dataset in a pandas DataFrame.\"\n",
        "\n",
        "3. **Instruction**: Get unique values of each column and show the first few rows of the dataset.\n",
        "   **Prompt**: \"Write Python code to print the unique values of each column and display the first 5 rows of a pandas DataFrame.\"\n",
        "\n",
        "4. **Instruction**: Create and plot a bar chart showing the distribution of the target variable \"Risk\".\n",
        "   **Prompt**: \"Write Python code using Plotly to create a bar chart showing the distribution of the 'Risk' column in a dataset.\"\n",
        "\n",
        "5. **Instruction**: Create and plot a histogram showing the distribution of the 'Age' column.\n",
        "   **Prompt**: \"Write Python code using Plotly to plot histograms for the 'Age' column grouped by 'good' and 'bad' risk categories.\"\n",
        "\n",
        "6. **Instruction**: Use Seaborn to create distribution and count plots of the 'Age' column, split by 'Risk'.\n",
        "   **Prompt**: \"Write Python code using Seaborn to create distribution and count plots of the 'Age' column, colored by 'Risk'.\"\n",
        "\n",
        "7. **Instruction**: Categorize the 'Age' column into groups and create box plots to compare 'Credit Amount' by age group and risk.\n",
        "   **Prompt**: \"Write Python code to categorize 'Age' into bins and create box plots comparing 'Credit Amount' across age groups and risk categories using Plotly.\"\n",
        "\n",
        "8. **Instruction**: Create a bar plot showing the distribution of the 'Housing' column, grouped by 'Risk'.\n",
        "   **Prompt**: \"Write Python code using Plotly to create bar plots showing the distribution of the 'Housing' column, grouped by 'Risk'.\"\n",
        "\n",
        "9. **Instruction**: Create a violin plot showing the distribution of 'Credit Amount' by 'Housing' and risk.\n",
        "   **Prompt**: \"Write Python code using Plotly to create a violin plot for 'Credit Amount' by 'Housing' and risk categories.\"\n",
        "\n",
        "10. **Instruction**: Create a bar plot and box plots comparing the distribution of 'Sex' and 'Credit Amount' across risk categories.\n",
        "    **Prompt**: \"Write Python code using Plotly to create bar and box plots comparing the distribution of 'Sex' and 'Credit Amount' across risk categories.\"\n",
        "\n",
        "11. **Instruction**: Create plots comparing the distribution of 'Job' and 'Credit Amount' across risk categories.\n",
        "    **Prompt**: \"Write Python code using Plotly to create bar and box plots comparing 'Job' distribution and 'Credit Amount' across risk categories.\"\n",
        "\n",
        "12. **Instruction**: Visualize the correlation between all the features in the dataset.\n",
        "    **Prompt**: \"Write Python code to plot a heatmap of the correlation matrix of a dataset using Seaborn.\"\n",
        "\n",
        "13. **Instruction**: Perform feature engineering by converting categorical columns into dummy variables.\n",
        "    **Prompt**: \"Write Python code to convert categorical columns into dummy variables using pandas' get_dummies() function.\"\n",
        "\n",
        "14. **Instruction**: Split the data into training and testing sets.\n",
        "    **Prompt**: \"Write Python code to split the data into training and testing sets using scikit-learn's train_test_split.\"\n",
        "\n",
        "15. **Instruction**: Implement several machine learning models (Logistic Regression, Random Forest, etc.) and evaluate their performance using cross-validation.\n",
        "    **Prompt**: \"Write Python code to implement multiple machine learning models like Logistic Regression, Random Forest, and evaluate them using cross-validation.\"\n",
        "\n",
        "16. **Instruction**: Fine-tune the hyperparameters of a Random Forest model using GridSearchCV.\n",
        "    **Prompt**: \"Write Python code to perform hyperparameter tuning for a Random Forest model using scikit-learn's GridSearchCV.\"\n",
        "\n",
        "17. **Instruction**: Plot the ROC curve for the model's predictions.\n",
        "    **Prompt**: \"Write Python code to plot the ROC curve for the predictions of a model using Matplotlib.\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VRv6LAMGzNkg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y620-hAczMsS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
