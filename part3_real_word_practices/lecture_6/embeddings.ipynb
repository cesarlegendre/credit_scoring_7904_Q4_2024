{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Word Embeddings in Large Language Models\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In natural language processing (NLP), **word embeddings** are numerical representations of words in a continuous vector space. These vectors capture semantic and syntactic information, allowing words with similar meanings to have similar representations. Word embeddings are fundamental to many NLP tasks and are extensively used in large language models (LLMs) to understand and generate human-like text.\n",
        "\n",
        "In this notebook, we will:\n",
        "\n",
        "- Explain the concept of embeddings in LLMs.\n",
        "- Load an embedding model from Hugging Face.\n",
        "- Compute embeddings for a list of 20 words.\n",
        "- Compute the similarities between words.\n",
        "\n",
        "## What are Word Embeddings?\n",
        "\n",
        "Traditional NLP techniques represented words as unique identifiers (one-hot encoding), which does not capture any semantic relationships between words. Word embeddings overcome this limitation by mapping words into a dense vector space where semantically similar words are positioned closely together.\n",
        "\n",
        "Embeddings are learned from large text corpora using techniques like Word2Vec, GloVe, or through transformer-based models. These embeddings enable models to perform tasks like sentiment analysis, machine translation, and question-answering more effectively.\n",
        "\n",
        "## Loading an Embedding Model from Hugging Face\n",
        "\n",
        "We will use the `sentence-transformers` library, which provides easy access to pre-trained models for computing embeddings. Specifically, we'll use the `'all-MiniLM-L6-v2'` model available on Hugging Face.\n",
        "\n",
        "### Install Dependencies\n"
      ],
      "metadata": {
        "id": "6RJ-Ojh85cM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers"
      ],
      "metadata": {
        "id": "r-5skA4Q5sx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### Load the Pre-trained Model\n",
        "\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('jinaai/jina-embeddings-v2-small-en')"
      ],
      "metadata": {
        "id": "4rDfmcTh5xq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing Embeddings for a List of Words\n",
        "\n",
        "Let's define a list of 20 words for which we'll compute embeddings."
      ],
      "metadata": {
        "id": "M7K8aO72538p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of words\n",
        "words = [\n",
        "    'dog', 'cat', 'apple', 'orange', 'car', 'bicycle', 'university', 'school',\n",
        "    'happy', 'sad', 'king', 'queen', 'man', 'woman', 'city', 'village',\n",
        "    'computer', 'phone', 'music', 'art'\n",
        "]"
      ],
      "metadata": {
        "id": "NdqY1t4H52xI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Compute Embeddings\n",
        " In many NLP applications and is a key component of large language models."
      ],
      "metadata": {
        "id": "DKaQVR7o5tQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute embeddings for the list of words\n",
        "embeddings = model.encode(words)"
      ],
      "metadata": {
        "id": "-iAN3LgT6L3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.shape"
      ],
      "metadata": {
        "id": "4cjrcgGT6Yjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Computing Similarities Between Words\n",
        "\n",
        "We can compute the cosine similarity between word embeddings to quantify how similar two words are semantically.\n",
        "\n"
      ],
      "metadata": {
        "id": "PQDphFIh6WS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "embeddings_dict = dict(zip(words, embeddings))\n",
        "\n",
        "\n",
        "def compute_similarity(word1, word2, embeddings_dict):\n",
        "    emb1 = embeddings_dict[word1].reshape(1, -1)\n",
        "    emb2 = embeddings_dict[word2].reshape(1, -1)\n",
        "    similarity = cosine_similarity(emb1, emb2)[0][0]\n",
        "    return similarity\n",
        "\n",
        "\n",
        "word_pairs = [\n",
        "    ('king', 'queen'),\n",
        "    ('king', 'man'),\n",
        "    ('king', 'phone'),\n",
        "    ('king', 'village'),\n",
        "    ('king', 'bicycle'),\n",
        "]"
      ],
      "metadata": {
        "id": "WOroPXSY6tqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word1, word2 in word_pairs:\n",
        "    similarity = compute_similarity(word1, word2, embeddings_dict)\n",
        "    print(f\"Similarity between '{word1}' and '{word2}': {similarity:.4f}\")"
      ],
      "metadata": {
        "id": "EWnbdmMC7L2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "As seen from the output, words that are semantically related have higher similarity scores.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "In this notebook, we've explored how word embeddings represent words in a continuous vector space, capturing semantic relationships. By computing embeddings for a list of words and calculating their cosine similarities, we've demonstrated how embeddings can quantify semantic similarities between words."
      ],
      "metadata": {
        "id": "fwrkexxc7rrE"
      }
    }
  ]
}
