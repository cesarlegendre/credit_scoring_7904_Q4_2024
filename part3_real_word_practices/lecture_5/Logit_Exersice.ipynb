{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression Exercise\n",
        "\n",
        "In this notebook, we will use the Breast Cancer dataset from scikit-learn to create a logistic regression model that classifies if a tumor is benign or malignant.\n",
        "\n",
        "## Objectives\n",
        "- Understand the steps to create a logistic regression model.\n",
        "- Interpret the model's parameters and evaluate its performance.\n",
        "- Apply feature engineering and preprocessing methods.\n",
        "\n",
        "## Step 1: Import Libraries"
      ],
      "metadata": {
        "id": "Lc6NLGMiYSJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n"
      ],
      "metadata": {
        "id": "MCa3U2WjYX6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Load the Dataset\n",
        "We will load the Breast Cancer dataset directly from scikit-learn."
      ],
      "metadata": {
        "id": "TzCJqPV9Yaad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Breast Cancer dataset\n",
        "cancer_data = load_breast_cancer()\n",
        "\n",
        "# Convert to a DataFrame for easier manipulation\n",
        "data = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)\n",
        "data['target'] = cancer_data.target\n",
        "\n",
        "# Show the first few rows of the dataset\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "QQHl7kCvYeAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Data Preprocessing\n",
        "## 3.1 Understand the Data\n",
        "The dataset contains features like the mean, standard error, and worst measurements of different characteristics (e.g., radius, texture, perimeter) for each tumor."
      ],
      "metadata": {
        "id": "jcRiRaTkZCzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the target distribution\n",
        "data['target'].value_counts()\n"
      ],
      "metadata": {
        "id": "i7g6FcTaZFDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Features and Target Variable\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kr4LUMQkZItB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Features (X) and target variable (y)\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n"
      ],
      "metadata": {
        "id": "OOWyyLw6ZG87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Split the Data into Training and Test Sets"
      ],
      "metadata": {
        "id": "jmwy84eNZSEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data: 70% for training and 30% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "usUt4FxgZV7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Feature Scaling\n",
        "Standardize the features using StandardScaler to ensure all variables are on a similar scale."
      ],
      "metadata": {
        "id": "aoqijrXhZTrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "3b1aovyPZf8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Train the Logistic Regression Model\n",
        "We'll train a logistic regression model using scikit-learn."
      ],
      "metadata": {
        "id": "2YnKnndpZlb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the logistic regression model\n",
        "log_reg = LogisticRegression(max_iter=10000)  # Increase iterations if convergence warning\n",
        "\n",
        "# Train the model with the training data\n",
        "log_reg.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "TfIPAuvpZk5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Model Parameters\n",
        "We can initialize the model with specific parameters if desired:\n",
        "\n",
        "*  C: Controls regularization strength.\n",
        "*  solver: The optimization algorithm used for finding the best weights.\n",
        "\n"
      ],
      "metadata": {
        "id": "fvmQ5s35ZuTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the logistic regression model with specific parameters\n",
        "log_reg = LogisticRegression(C=1.0, solver='liblinear', max_iter=10000)\n",
        "log_reg.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "CkvOSXKTZ6sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Make Predictions\n"
      ],
      "metadata": {
        "id": "hVTSbrLdZ_Qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n"
      ],
      "metadata": {
        "id": "OR7wh9mVZ911"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Evaluate the Model\n",
        "## 9.1 Confusion Matrix"
      ],
      "metadata": {
        "id": "hruUceC9aHlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(conf_matrix)\n",
        "\n",
        "# Plotting the confusion matrix\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OxyRch-nZqjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.2 Classification Report and accuracy\n"
      ],
      "metadata": {
        "id": "aa5HBBPCaaG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "GBy_iyrnaYd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "JaM1UTbFaP76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Interpret the Results\n",
        "\n",
        "- **Confusion Matrix**: Shows the number of true positives, true negatives, false positives, and false negatives.\n",
        "- **Classification Report**: Provides metrics like precision, recall, and F1-score for each class.\n",
        "- **Accuracy Score**: The proportion of correct predictions.\n",
        "\n",
        "### Key Points\n",
        "\n",
        "- Logistic Regression is a strong baseline model for binary classification problems.\n",
        "- **Regularization** is crucial for controlling model complexity and avoiding overfitting.\n",
        "- **Feature scaling** is a key preprocessing step for logistic regression.\n",
        "\n",
        "## Step 11: Improving the Model (Optional)\n",
        "\n",
        "- Adjust the **C** parameter to see how regularization affects performance.\n",
        "- Use **cross-validation** to obtain a more robust estimate of model performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "qoA2dKXYatld"
      }
    }
  ]
}
