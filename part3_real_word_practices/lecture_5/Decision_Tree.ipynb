{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree Classifier Exercise\n",
        "\n",
        "This notebook will guide you through implementing a Decision Tree classifier using Python's popular `scikit-learn` library. We will use the same dataset generated previously with overlapping classes, making it a great exercise to understand the workings of decision trees.\n",
        "\n",
        "## Objectives\n",
        "- Understand how to build and visualize a Decision Tree classifier.\n",
        "- Learn how to interpret the decision tree and its structure.\n",
        "- Apply model evaluation techniques to assess performance."
      ],
      "metadata": {
        "id": "eU3hM9RLmPPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
      ],
      "metadata": {
        "id": "HQRLNDPemVcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
      ],
      "metadata": {
        "id": "FLTebTVxmZZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 2: Prepare the Dataset\n",
        "\n",
        "We will use the same generated dataset with overlapping classes.\n"
      ],
      "metadata": {
        "id": "r8XF-SGvmc9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the data again (Red and Green points with overlap)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Red points (class 1) - overlapping with green\n",
        "x_red = np.random.uniform(2, 7, 20)\n",
        "y_red = np.random.uniform(2, 8, 20)\n",
        "\n",
        "# Green points (class 2) - overlapping with red\n",
        "x_green = np.random.uniform(4, 9, 20)\n",
        "y_green = np.random.uniform(4, 10, 20)\n",
        "\n",
        "# Create a DataFrame for the data\n",
        "data_dict = {\n",
        "    'Feature 1': np.concatenate([x_red, x_green]),\n",
        "    'Feature 2': np.concatenate([y_red, y_green]),\n",
        "    'Class': [0] * len(x_red) + [1] * len(x_green)  # 0: Red, 1: Green\n",
        "}\n",
        "data_df = pd.DataFrame(data_dict)\n",
        "\n",
        "# Show the first few rows\n",
        "data_df.head()"
      ],
      "metadata": {
        "id": "X0UofJDFmgc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 3: Data Preprocessing\n",
        "\n",
        "### 3.1 Features and Target Variables\n",
        "We need to separate the features and the target variable before training the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "C__WdYHymn6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Features (X) and target variable (y)\n",
        "X = data_df[['Feature 1', 'Feature 2']]\n",
        "y = data_df['Class']\n"
      ],
      "metadata": {
        "id": "PAOvCHujmr0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3.2 Split the Data into Training and Test Sets\n",
        "We will split the dataset into training and testing sets with a ratio of 70/30.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vB988id-mrJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split the data: 70% for training and 30% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "5wKkdnLTmxbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Step 4: Train the Decision Tree Model\n",
        "We will train a Decision Tree classifier using `scikit-learn`.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fg2sDl9dm0S0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the decision tree model\n",
        "tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)  # Limiting depth to avoid overfitting\n",
        "\n",
        "# Train the model with the training data\n",
        "tree_clf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "h2XgS0Vqm3Io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Step 5: Visualize the Decision Tree\n",
        "Plotting the decision tree helps us understand its structure and how it splits the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "uAXGx6tcm7Qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plotting the decision tree\n",
        "plt.figure(figsize=(15, 5))\n",
        "plot_tree(tree_clf, feature_names=['Feature 1', 'Feature 2'], class_names=['Red', 'Green'], filled=True)\n",
        "plt.title('Decision Tree Visualization')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yuwByhCbm8bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "## Step 6: Make Predictions\n",
        "We will now make predictions on the test set to evaluate the performance of the model.\n"
      ],
      "metadata": {
        "id": "xQabyaWanN7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = tree_clf.predict(X_test)\n"
      ],
      "metadata": {
        "id": "bblwrdoNnPD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Evaluate the Model\n",
        "### 7.1 Confusion Matrix\n"
      ],
      "metadata": {
        "id": "N6tBCfUGnSUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(conf_matrix)\n",
        "\n",
        "# Plotting the confusion matrix\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oHiY17bsnVdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Classification Report"
      ],
      "metadata": {
        "id": "hMGaZvW7nUqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "ZV1lKmYendPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 7.3 Accuracy Score\n"
      ],
      "metadata": {
        "id": "76mYkN6xngZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ElPsXL_anjCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Interpret the Results\n",
        "- The **decision tree visualization** shows the sequence of splits that are performed on the features to classify the points.\n",
        "- The **confusion matrix** indicates how well the model performed in predicting the correct classes.\n",
        "- The **classification report** provides metrics like precision, recall, and F1-score for each class.\n",
        "- **Accuracy score** shows the percentage of correct predictions.\n",
        "\n",
        "## Key Points\n",
        "- Decision Trees are very intuitive and easy to visualize.\n",
        "- **Depth of the tree** is crucial for controlling overfitting. Limiting depth helps the model generalize better.\n",
        "- Decision Trees are powerful but can easily overfit, especially if there are no depth or minimum sample requirements.\n",
        "\n",
        "## Step 9: Improving the Model (Optional)\n",
        "- **Increase or Decrease Tree Depth**: Adjusting the `max_depth` parameter can help improve performance.\n",
        "- **Feature Engineering**: Adding more features could lead to better splits.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aeNCQlJvmK_0"
      }
    }
  ]
}
