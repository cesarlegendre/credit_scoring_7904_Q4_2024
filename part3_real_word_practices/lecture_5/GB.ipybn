{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Gradient Boosting Classifier Exercise with Wine Dataset\n",
        "\n",
        "This notebook will guide you through implementing a Gradient Boosting classifier using Python's popular `scikit-learn` library. We will use the `load_wine` dataset, which contains chemical measurements of wines, to demonstrate the capabilities of the Gradient Boosting model.\n",
        "\n",
        "## Objectives\n",
        "- Understand how to build a Gradient Boosting classifier.\n",
        "- Learn how to interpret the model's feature importance.\n",
        "- Apply model evaluation techniques to assess performance.\n"
      ],
      "metadata": {
        "id": "a3CpRotZt9ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "T4mmtphzt-dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load the Wine Dataset\n",
        "\n",
        "We will use the `load_wine` dataset from `scikit-learn`. The dataset contains measurements for different wines derived from three different cultivars.\n"
      ],
      "metadata": {
        "id": "RymU4KP8t-k1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load wine dataset from sklearn\n",
        "wine = datasets.load_wine()\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "data = pd.DataFrame(data=wine.data, columns=wine.feature_names)\n",
        "data['target'] = wine.target\n",
        "\n",
        "# Show the first few rows of the dataset\n",
        "data.head()"
      ],
      "metadata": {
        "id": "3_511tVFt-ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Split the Data into Training and Test Sets\n",
        "\n",
        "We will split the dataset into training and testing sets using a 70/30 ratio.\n"
      ],
      "metadata": {
        "id": "wG8P-leYt-xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Features (X) and target variable (y)\n",
        "X = data[wine.feature_names]\n",
        "y = data['target']\n",
        "\n",
        "# Split the dataset into training and testing sets (70% for training, 30% for testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "WwmTip2Xt-3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 4: Train the Gradient Boosting Model\n",
        "\n",
        "We will train a Gradient Boosting classifier using the following important parameters:\n",
        "- **n_estimators**: Number of boosting stages.\n",
        "- **learning_rate**: Step size that controls the contribution of each tree.\n",
        "- **max_depth**: Maximum depth of the individual trees to control complexity.\n",
        "- **random_state**: Ensures reproducibility of the results.\n",
        "\n"
      ],
      "metadata": {
        "id": "HlZQ5uAVt-9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the Gradient Boosting model\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "UCbR6ra9t_Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Step 5: Feature Importance\n",
        "\n",
        "One of the key advantages of Gradient Boosting is its ability to rank feature importance.\n"
      ],
      "metadata": {
        "id": "QS6KN7uPt_J9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importance\n",
        "feature_importances = gb_clf.feature_importances_\n",
        "\n",
        "# Create a DataFrame to visualize feature importance\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': wine.feature_names,\n",
        "    'Importance': feature_importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plotting feature importance\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
        "plt.title('Feature Importance in Gradient Boosting')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9JqJdkgjt_PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Step 6: Make Predictions and Evaluate the Model\n",
        "\n",
        "We will make predictions on the test set and evaluate the modelâ€™s performance using metrics like accuracy, classification report, and confusion matrix.\n",
        "\n"
      ],
      "metadata": {
        "id": "MXfOFkJKt_Ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = gb_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Plot the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=wine.target_names, yticklabels=wine.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "09ZtNDv9t_ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Interpretation of the Results\n",
        "\n",
        "- **Feature Importance**: The plot shows the features ranked by their importance. It allows us to understand which measurements have the most influence on classifying different wine cultivars.\n",
        "- **Confusion Matrix**: The confusion matrix provides a clear picture of the model's classification results, highlighting correct and incorrect predictions.\n",
        "- **Accuracy and Classification Report**: The accuracy score and classification report provide insight into the model's precision, recall, and overall performance.\n",
        "\n",
        "## Key Points\n",
        "\n",
        "- **n_estimators**: Defines the number of boosting stages. A larger value generally results in better accuracy but may require more computation.\n",
        "- **learning_rate**: This parameter determines the contribution of each tree to the final model. Smaller values make the model more robust by requiring more trees.\n",
        "- **max_depth**: Controlling the depth of each tree helps balance between model complexity and overfitting.\n",
        "\n",
        "### Final Thoughts\n",
        "\n",
        "Gradient Boosting is a powerful machine learning algorithm that combines multiple weak learners to create a strong learner. It is especially effective for classification and regression tasks where slight overfitting is acceptable but model accuracy is crucial.\n",
        "\n",
        "Feel free to experiment with other hyperparameters like **min_samples_split** or **subsample** to see how they impact the model's performance.\n"
      ],
      "metadata": {
        "id": "ztl0jZImt_gN"
      }
    }
  ]
}
